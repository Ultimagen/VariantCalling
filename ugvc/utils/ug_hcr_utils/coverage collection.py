import pandas as pd
import re
import json
from os.path import join as pjoin
import os
import subprocess
import pyBigWig as pbw
import numpy as np
import warnings
warnings.filterwarnings('ignore')

def collect_parquets_files(mds):
    """
    collect parquet files generated by singleSampleQC pipeline.
    Parameters
    ----------
    mds - list of metadata json files of SSQC workflow on gs://.

    Returns
    -------
    2 lists:
    - parquets_mapq0 - list of parquet files of mapq0
    - parquets_mapq20 - list of parquets files of mapq20
    list contain coverage collection using several window sizes.
    """
    parquets_mapq0 = []
    parquets_mapq20 = []

    for m in mds:
        cmd = 'gsutil cp ' + m + ' .'
        os.system(cmd)
        md = json.load(open('metadata.json'))
        pqs = md['calls']['SingleSampleQC.CollectIntervalCoverageStatsMappingQuality0'][-1]['outputs']['coverage_parquet']
        parquets_mapq0.append(pqs)
        pqs = md['calls']['SingleSampleQC.CollectIntervalCoverageStatsMappingQuality20'][-1]['outputs']['coverage_parquet']
        parquets_mapq20.append(pqs)
        cmd = 'rm -f metadata.json'
        os.system(cmd)
    return parquets_mapq0,parquets_mapq20

def collect_bw_file(mds):
    """
    collect bigwig files generated by singleSampleQC pipeline.
    Parameters
    ----------
    mds - list of metadata json files of SSQC workflow on gs://.

    Returns
    -------
    2 lists:
    - bw_mapq0 - list of bigwig files of mapq0
    - bw_mapq20 - list of bigwig files of mapq20
    """
    bw_mapq0 = []
    bw_mapq20 = []

    for m in mds:
        cmd = 'gsutil cp ' + m + ' .'
        os.system(cmd)

        md = json.load(open('metadata.json'))
        bw = md['calls']['SingleSampleQC.CollectIntervalCoverageStatsMappingQuality0'][-1]['outputs']['coverage_bw']
        bw_mapq0.append(bw)
        bw = md['calls']['SingleSampleQC.CollectIntervalCoverageStatsMappingQuality20'][-1]['outputs']['coverage_bw']
        bw_mapq20.append(bw)

        cmd = 'rm -f metadata.json'
        os.system(cmd)
        return bw_mapq0,bw_mapq20

def flatten(l):
    flatten_list=[]
    for element in l:
        for f in element:
            flatten_list.append(f)
    return flatten_list

def fetch_w100_parquets_list(parquets):
    """
    filter parquets files list to w100 only.
    Parameters
    ----------
    parquets - list of parquet files

    Returns
    -------
    list of coverage parquet files binned to 100b windows.
    """
    parquets = [ [ y for y in x if '.w100.' in y] for x in parquets]
    parquets = [ [ y for y in x if '_alt' not in y] for x in parquets]
    parquets = [ [ y for y in x if 'random' not in y] for x in parquets]
    return parquets


def read_parquet_files_to_dataframe(parquet_files_list,out_dir):
    """
    read all samples parquet files into a single dataframe
    Parameters
    ----------
    parquets_files_list - parquet file for each input sample
    out_dir - output directory for intermediate files

    Returns
    -------
    dataframe with windows columns ['chrom','chromStart','chromEnd'] and a column for each sample in the input parquets list.
    """
    current_basename=os.path.basename(parquet_files_list[0]).split(".",1)[0]
    df_all_samples=pd.DataFrame(columns=['chrom','chromStart','chromEnd'])
    df_sample = pd.DataFrame(columns=['chrom','chromStart','chromEnd',current_basename])

    for file in parquet_files_list:
        if file == '':
            continue

        cmd = 'gsutil cp ' + file + ' ' + out_dir
        os.system(cmd)

        filename= pjoin(out_dir,os.path.basename(file))
        basename=os.path.basename(file).split(".",1)[0]
        df_parquet = pd.read_parquet(filename)
        df_parquet.rename(columns = {'coverage':basename}, inplace = True)
        if basename == current_basename:
            df_sample = pd.concat([df_sample,df_parquet])
        else:
            df_all_samples = pd.merge(df_all_samples,df_sample,on=['chrom','chromStart','chromEnd'],how="right")
            current_basename = basename
            df_sample = df_parquet

        cmd = 'rm -f ' + filename
        os.system(cmd)
    df_all_samples = pd.merge(df_all_samples,df_sample,on=['chrom','chromStart','chromEnd'],how="right")
    return df_all_samples

def get_mean_and_medians_by_chr(df_all_samples):
    """
    calculate mean and median coverage per sample per chromosome
    Parameters
    ----------
    df_all_samples - dataframe with (normalized) coverage per windows with all samples.

    Returns
    -------
    dataframe of medians and dataframe of average coverage per chromosome per sample.
    """
    df_medians_by_chr = pd.DataFrame()
    df_mean_by_chr = pd.DataFrame()
    for column in df_all_samples.columns:
        if column in ['chrom','chromStart','chromEnd']:
            continue
        else:
            df_medians_by_chr[column] = df_all_samples.groupby('chrom',sort=False)[column].median()
            df_mean_by_chr[column]=df_all_samples.groupby('chrom',sort=False)[column].mean()
    df_mean_by_chr['chrom'] = df_mean_by_chr.index
    df_medians_by_chr['chrom'] = df_medians_by_chr.index
    return [df_medians_by_chr,df_mean_by_chr]

def normalize_by_mean(df_all_samples):
    """
    normalize binned coverage by mean.
    Parameters
    ----------
    df_all_samples - dataframe holding all samples coverage per window

    Returns
    -------
    dataframe holding normalized coverage (by mean) per window per sample.

    """
    df_all_samples_norm = pd.DataFrame()
    for column in df_all_samples.columns:
        if column in ['chrom','chromStart','chromEnd']:
            df_all_samples_norm[column] = df_all_samples[column]
        else:
            df_all_samples_norm[column] = df_all_samples.groupby('chrom',sort=False)[column].transform(lambda x:(x/x.mean())*50)
    return df_all_samples_norm

def big_wig_to_binned_coverage(bw_file, output_dir, sample_name, interval_length):
    """
    convert input bigwig file into mean coverage binned to windows
    Parameters
    ----------
    bw_file - bigwig file
    output_dir - output directory
    sample_name - sample name
    interval_length - window size

    Returns
    -------
    writes h5 file into output_dir/coverage_<sample_name>.h5
    keys are chromosomes and each key holds dataframe with the following columns: ['chrom','start','end','coverage']
    coverage is calculates as the mean coverage in the window.
    """
    cmd = f"gsutil cp {bw_file} {output_dir}"
    subprocess.check_call(cmd, shell=True)
    bw = pbw.open(pjoin(output_dir, os.path.basename(bw_file)))
    print('done download')
    r = re.compile(r'(chr[0-9XYM]*)\.q0')
    chrom = r.search(os.path.basename(bw_file)).group(1)
    chrom_size = bw.chroms(chrom)
    cov_means = np.array(bw.stats(chrom, 1, chrom_size, type="mean", nBins=int(chrom_size / interval_length)))
    df = pd.DataFrame({"chrom": chrom,
                       "start": range(1, interval_length * len(cov_means), interval_length),
                       "end": range(interval_length, interval_length * (len(cov_means) + 1), interval_length),
                       "coverage": cov_means})
    df.to_hdf(os.path.join(output_dir, "coverage_" + sample_name + ".h5"), key=chrom)
    os.remove(pjoin(output_dir, os.path.basename(bw_file)))

def read_and_norm(file, chrom):
    """
    reads binned coverage data and normalize.
    Parameters
    ----------
    file - h5 file resulted from big_wig_to_binned_coverage holding coverage information per sample.
    chrom - chromosome that will be used as key to read from h5.

    Returns
    -------
    dataframe with normalized coverage per sample per chromosome(50*coverage/mean)

    """
    df = pd.read_hdf(file, key=chrom) \
           .assign(file=os.path.basename(file)) \
           .assign(norm_cov = lambda df: df["coverage"]/np.mean(df["coverage"], axis=0)*50)
    return df


def normalize_binned_coverage(output_file,h5_files,chroms):
    """
    normalize binned coverage data for all input samples.
    Parameters
    ----------
    output_file - output file - h5 file that will hold coverage mean and std for each window.
    h5_files - list of h5 files representing all input samples.
    chroms - list of chromosomes.

    Returns
    -------
    writes h5 file into output_dir/coverage_<sample_name>.h5
    keys are chromosomes and each key holds dataframe with the following columns: ['chrom','start','end','mean','std']
    coverage is calculates as the mean coverage along all normalized samples for each window.
    std is calculates as the std along all normalized samples for each window.
    """
    for chrom in chroms:
        print(chrom)
        df = pd.concat([read_and_norm(f, chrom) for f in h5_files], ignore_index=True)
        agg_df = df.groupby(["chrom", "start", "end"])["norm_cov"] \
                   .agg(["mean","std"])
        agg_df.to_hdf(output_file, key=chrom)
        del df
        del agg_df

def calculate_sliding_window(step, original_df):
    """
    calculates sliding windows on non-overlapping windows input.
    Parameters
    ----------
    step - number of windows to move forward
    original_df - dataframe of non-overlapping windows with the following columns : ['chrom','start','end','coverage']

    Returns
    -------
    a dataframe of sliding windows with the following columns : ['chrom','start','end','coverage'].
    coverage is calculates as mean of all windows in step.
    """
    df1 = pd.DataFrame()

    df1['chrom'] = original_df['chrom']

    df_start = pd.DataFrame()
    df_start['start'] = original_df['start']
    df1['start'] = df_start.rolling(step).min()

    df_start = pd.DataFrame()
    df_start['end'] = original_df['end']
    df1['end'] = df_start.rolling(step).max()

    df_cov = pd.DataFrame()
    df_cov['coverage'] = original_df['coverage']
    df1['coverage'] = df_cov.rolling(step).mean()

    return df1.dropna()

def read_slide_and_norm(file, chrom, step):
    """
    create sliding window dataframe and normalize per sample per chromosome.
    Parameters
    ----------
    file - h5 file holding coverage information for non-overlapping windows.
    chrom  - chromosome that will be used as key for reading h5 file
    step - number of windows to move forward for sliding window

    Returns
    -------
    dataframe of sliding windows with corresponding average coverage per window for the given chromosome and sample.

    """
    df = pd.read_hdf(file, key=chrom)
    df_sliding = calculate_sliding_window(step, df) \
        .assign(file=os.path.basename(file)) \
        .assign(norm_cov=lambda df_sliding: df_sliding["coverage"] / np.mean(df_sliding["coverage"], axis=0) * 50)
    return df_sliding


