{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant Calling Report v1.2.6\n",
    "## 1. Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nexusplt as nxp\n",
    "from configparser import ConfigParser\n",
    "\n",
    "pd.options.display.float_format = '{:,.2%}'.format\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFile='var_report.config'\n",
    "parser = ConfigParser()\n",
    "parser.read(configFile)\n",
    "\n",
    "prmNames=['run_id','pipeline_version', \n",
    "          'h5_concordance_file', 'h5_model_file',\n",
    "          'model_name_with_gt','model_name_without_gt','model_pkl_with_gt','model_pkl_without_gt'\n",
    "         ]\n",
    "\n",
    "prm={}\n",
    "for name in prmNames:\n",
    "    prm[name]=parser.get('VarReport', name)\n",
    "\n",
    "h5outfile = parser.get('VarReport', 'h5_output', fallback='var_report.h5')\n",
    "imgpref = parser.get('VarReport', 'image_output_prefix', fallback=prm['run_id']+'.vars')+'.'\n",
    "imgdir = 'plots'\n",
    "\n",
    "sources = {'Trained wo gt':(prm['h5_concordance_file'],\"concordance\"),\n",
    "           'Trained with gt':(prm['h5_model_file'],\"scored_concordance\"),\n",
    "          }\n",
    "\n",
    "data = {}\n",
    "\n",
    "for s in sources:\n",
    "    data[s]={}\n",
    "    d=pd.read_hdf(sources[s][0], key=sources[s][1], mode='r')\n",
    "    data[s]=d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = ['indel','hmer_indel_length', 'tree_score',\n",
    "                     'filter','blacklst', 'classify','classify_gt',\n",
    "                     'indel_length','hmer_indel_nuc','ref',\n",
    "                     'gt_ground_truth','well_mapped_coverage','mappability.0','ug_hcr','LCR-hs38']\n",
    "\n",
    "# Load the concordance data for the entire genome\n",
    "with pd.HDFStore(prm['h5_concordance_file']) as hdf:\n",
    "    keys=hdf.keys()\n",
    "    wg_dfs=[pd.read_hdf(hdf,k) for k in keys if k not in ['/concordance','/input_args']]\n",
    "    wg_dfs = [ w[[ x for x in columns_to_select if x in w.columns]] for w in wg_dfs]\n",
    "    wg_df=pd.concat(wg_dfs)\n",
    "    \n",
    "data['whole genome'] = wg_df\n",
    "sources['whole genome'] = (prm['h5_concordance_file'],\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'well_mapped_coverage' in data['whole genome'].columns:\n",
    "    prm['mean_var_depth']='{:.2f}'.format(data['whole genome']['well_mapped_coverage'].mean())\n",
    "    prmNames.append('mean_var_depth')\n",
    "   \n",
    "try:\n",
    "    args=pd.read_hdf(sources['Trained wo gt'][0], 'input_args', mode='r')\n",
    "    prm['truth_sample_name']=args['truth_sample_name'][0]\n",
    "except:\n",
    "    prm['truth_sample_name']=parser.get('VarReport', 'truth_sample_name', fallback='NA')\n",
    "prmNames.append('truth_sample_name')\n",
    "\n",
    "\n",
    "prmdf = pd.DataFrame.from_dict(prm, orient='index',columns=['value']).reindex(prmNames)\n",
    "prmdf.to_hdf(h5outfile, key=\"parameters\")\n",
    "\n",
    "prmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filterByCategory(data,cat):\n",
    "    if cat=='SNP':\n",
    "        return data[data['indel']==False]\n",
    "    if cat=='Indel':\n",
    "        return data[data['indel']==True]\n",
    "    elif cat=='non-hmer Indel':\n",
    "        return data[(data['indel']==True) & (data['hmer_indel_length']==0) & (data['indel_length']>0)]\n",
    "    elif cat=='non-hmer Indel w/o LCR':\n",
    "        return data[(data['indel']==True) & (data['hmer_indel_length']==0) & (data['indel_length']>0) & \n",
    "                    (~data['LCR-hs38'])]\n",
    "    elif cat=='hmer Indel <=4':\n",
    "        return data[(data['indel']==True) & (data['hmer_indel_length']>0) & (data['hmer_indel_length']<=4)]\n",
    "    elif cat=='hmer Indel >4,<=8':\n",
    "        return data[(data['indel']==True) & (data['hmer_indel_length']>4) & (data['hmer_indel_length']<=8)]\n",
    "    elif cat=='hmer Indel >8,<=10':\n",
    "        return data[(data['indel']==True) & (data['hmer_indel_length']>8) & (data['hmer_indel_length']<=10)]\n",
    "    for i in range (1,10):\n",
    "        if cat=='hmer Indel {0:d}'.format(i):\n",
    "            return data[(data['indel']==True) & (data['hmer_indel_length']==i)]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPerformance(data):\n",
    "    classify_col='classify_gt' \n",
    "    d=data.copy()\n",
    "    \n",
    "    # Calculate precision and recall continuously along the tree_score values\n",
    "    d['tree_score'] = np.where(d[classify_col]=='fn',-1,d['tree_score'])\n",
    "    d=d[[classify_col,'tree_score','filter']].sort_values(by=['tree_score'])\n",
    "    \n",
    "    d['label'] = np.where(d[classify_col]=='fp',0,1)\n",
    "\n",
    "    d.loc[d['filter']=='HPOL_RUN','filter']='PASS'\n",
    "    d.loc[d[classify_col]=='fn','filter']='MISS'\n",
    "\n",
    "    num=len(d)\n",
    "    numPos=sum(d['label'])\n",
    "    numNeg=num-numPos\n",
    "    if num<10:\n",
    "        return (pd.DataFrame(),None,numPos,numNeg)\n",
    "    \n",
    "    d['fn']=np.cumsum(d['label'])\n",
    "    d['tp']=numPos-(d['fn'])\n",
    "    d['fp']=numNeg-np.cumsum(1-d['label'])\n",
    "\n",
    "    d['recall']=d['tp']/(d['tp']+d['fn'])\n",
    "    d['precision']=d['tp']/(d['tp']+d['fp'])\n",
    "\n",
    "    d['f1']=d['tp']/(d['tp']+0.5*d['fn']+0.5*d['fp'])\n",
    "\n",
    "    d['mask']=((d['tp']+d['fn'])>=20) & ((d['tp']+d['fp'])>=20) & (d['tree_score']>=0)\n",
    "    if len(d[d['mask']])==0:\n",
    "        return (pd.DataFrame(),None,numPos,numNeg)\n",
    "\n",
    "    # Calculate the precision and recall as ouputted by the model (based on the FILTER column)\n",
    "    d['class'] = np.where(d['label']==0,'FP','FN')\n",
    "    d.loc[(d['label']==1) & (d['filter']=='PASS'),'class']='TP'\n",
    "    d.loc[(d['label']==0) & (d['filter']!='PASS'),'class']='TN'\n",
    "\n",
    "    fn=len(d[d['class']=='FN'])\n",
    "    tp=len(d[d['class']=='TP'])\n",
    "    fp=len(d[d['class']=='FP'])\n",
    "\n",
    "    recall=tp/(tp+fn) if (tp+fn>0) else np.nan\n",
    "    precision=tp/(tp+fp) if (tp+fp>0) else np.nan\n",
    "    max_recall=1-len(d[d['filter']=='MISS'])/numPos\n",
    "\n",
    "    f1=tp/(tp+0.5*fn+0.5*fp)\n",
    "    \n",
    "    return (d[['recall','precision']][d['mask']],\n",
    "            dict({'recall':recall,'precision':precision,'f1':f1}),\n",
    "            numPos,numNeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerformance(perfCurve,optRes,categories,source=sources,ext=None,img=None,legend=None,optRes_SEC=None):\n",
    "    n=len(categories)\n",
    "    nrow=int(np.ceil(n/5))\n",
    "    ncol=int(max(5,n))\n",
    "    fig, ax = plt.subplots(1,n,figsize=(4*n,4))\n",
    "    #fig, ax = plt.subplots(1,ncol) #figsize=(4*nrow,4*ncol)\n",
    "    #ax=ax.flatten()\n",
    "    \n",
    "    col=['r','b','g','m','k']\n",
    "\n",
    "    for i,cat in enumerate(categories):\n",
    "        for j,s in enumerate(source):\n",
    "            perf=perfCurve[s][cat]\n",
    "            opt=optRes[s][cat]\n",
    "            if optRes_SEC is not None:\n",
    "                opt_sec=optRes_SEC[s][cat]\n",
    "            if not perf.empty:\n",
    "                ax[i].plot(perf.recall,perf.precision,'-',label=s,color=col[j])    \n",
    "                ax[i].plot(opt.get('recall'),opt.get('precision'),'o',color=col[j])\n",
    "                if optRes_SEC is not None:\n",
    "                    ax[i].plot(opt_sec.get('recall'),opt_sec.get('precision'),'o',color=\"black\")\n",
    "            title=cat if ext==None else '{0} ({1})'.format(cat,ext)\n",
    "            ax[i].set_title(title)\n",
    "            ax[i].set_xlabel(\"Recall\")\n",
    "            ax[i].set_xlim([0.4,1])\n",
    "            ax[i].set_ylim([0.4,1])\n",
    "            ax[i].grid(True)\n",
    "\n",
    "    ax[0].set_ylabel(\"Precision\")\n",
    "    if legend: \n",
    "        ax[0].legend(loc='lower left')    \n",
    "    if img:\n",
    "        nxp.save(fig,imgpref+img,'png',outdir=imgdir)\n",
    "    \n",
    "    \n",
    "def getPerformance(data,categories):\n",
    "    optTab={}\n",
    "    optRes={}\n",
    "    perfCurve={}\n",
    "    for s in sources:\n",
    "        optTab[s]=pd.DataFrame()\n",
    "        optRes[s]={}\n",
    "        perfCurve[s]={}\n",
    "\n",
    "        for i,cat in enumerate(categories):\n",
    "            d=filterByCategory(data[s],cat)\n",
    "            perf,opt,pos,neg=calcPerformance(d)\n",
    "            perfCurve[s][cat]=perf\n",
    "            optRes[s][cat]=opt\n",
    "            \n",
    "            row=pd.DataFrame({'# pos':pos,\n",
    "                              '# neg':neg,\n",
    "                              'max recall':np.nan if perf.empty else max(perf.recall),\n",
    "                              'recall':np.nan if perf.empty else opt.get('recall'),\n",
    "                              'precision':np.nan if perf.empty else opt.get('precision'),\n",
    "                              'F1':np.nan if perf.empty else opt.get('f1')\n",
    "                             },index=[cat])\n",
    "            optTab[s]=pd.concat([optTab[s],row])\n",
    "            \n",
    "    return optTab,optRes,perfCurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Performance over all Data\n",
    "The concordance between the variant calling results and the ground truth sample is presented below.\n",
    "* Red line - precision and recall over different tree-scores.\n",
    "* Red dot - precision and recall values for the chosen threshold.\n",
    "* Black dot -precision and recall after filtering systematic errors (SEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_sec(x):\n",
    "    res = False\n",
    "    if x is not None:\n",
    "        if x==x:\n",
    "            if \"SEC\" in x:\n",
    "                res=True\n",
    "    return res\n",
    "\n",
    "sec_df = data['whole genome'].copy()\n",
    "data_SEC={}\n",
    "if 'blacklst' in sec_df.columns:\n",
    "    is_sec = sec_df['blacklst'].apply(has_sec)\n",
    "    sec_df.loc[is_sec,'filter'] = \"SEC\"\n",
    "    sec_df.loc[(is_sec) & (sec_df['classify_gt']=='tp'),'classify_gt'] = \"fn\"\n",
    "    sec_df_new=sec_df[~((is_sec) & (sec_df['classify_gt']=='fp'))]\n",
    "    data_SEC={'whole genome': sec_df_new,'Trained wo gt':data['Trained wo gt'].copy(), 'Trained with gt':data['Trained with gt'].copy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "optTab1,optRes,perfCurve=getPerformance(data,categories)\n",
    "if data_SEC :\n",
    "    SEC_optTab1,SEC_optRes,SEC_perfCurve=getPerformance(data_SEC,categories)\n",
    "    plotPerformance(perfCurve,optRes,categories,img='all.primary',source={'whole genome':sources['whole genome']},optRes_SEC=SEC_optRes)\n",
    "else:\n",
    "    plotPerformance(perfCurve,optRes,categories,img='all.primary',source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['hmer Indel 4','hmer Indel 5','hmer Indel 6','hmer Indel 7','hmer Indel 8','hmer Indel >8,<=10']\n",
    "optTab2,optRes,perfCurve=getPerformance(data,categories)\n",
    "if data_SEC :\n",
    "    SEC_optTab2,SEC_optRes,SEC_perfCurve=getPerformance(data_SEC,categories)\n",
    "    plotPerformance(perfCurve,optRes,categories,img='all.primary',source={'whole genome':sources['whole genome']},optRes_SEC=SEC_optRes)\n",
    "else:\n",
    "    plotPerformance(perfCurve,optRes,categories,img='all.primary',source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2%}'.format\n",
    "\n",
    "optTab={}\n",
    "for s in sources:\n",
    "    optTab[s]=pd.concat([optTab1[s], optTab2[s]])\n",
    "\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"all_data\")\n",
    "\n",
    "if data_SEC :\n",
    "    df_SEC=pd.concat([SEC_optTab1['whole genome'],SEC_optTab2['whole genome']])\n",
    "    df_SEC.to_hdf(h5outfile, key=\"sec_data\")\n",
    "    display(pd.concat([df['whole genome'],df_SEC], keys=['Whole genome','After filtering systematic errors'], axis=1))\n",
    "else:\n",
    "    display(df['whole genome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Homozygous genotyping accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall of called homozygous variants (where the variant was not classfied as False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "\n",
    "hmzData={}\n",
    "for s in sources:\n",
    "        d=data[s]\n",
    "        hmzData[s]=d[(d['gt_ground_truth']==(1,1)) & (d['classify']!='fn')]\n",
    "optTab,optRes,perfCurve=getPerformance(hmzData,categories)\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"all_data_homozygous\")\n",
    "\n",
    "df['whole genome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Stratified by base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','hmer Indel <=4','hmer Indel >4,<=8','hmer Indel >8,<=10']\n",
    "\n",
    "baseData={}\n",
    "b =('A','T')\n",
    "for s in sources:\n",
    "        d=data[s]\n",
    "        baseData[s]=d[((d['indel']==False) & ((d['ref']==b[0]) | (d['ref']==b[1]))) |\n",
    "                      ((d['hmer_indel_length']>0) & ((d['hmer_indel_nuc']==b[0]) | (d['hmer_indel_nuc']==b[1])))\n",
    "                     ]\n",
    "optTab1,optRes,perfCurve=getPerformance(baseData,categories)\n",
    "for s in sources:\n",
    "    optTab1[s].rename(index={a:'{0} ({1}/{2})'.format(a,b[0],b[1]) for a in optTab1[s].index}, inplace=True)\n",
    "plotPerformance(perfCurve,optRes,categories,source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (G,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','hmer Indel <=4','hmer Indel >4,<=8','hmer Indel >8,<=10']\n",
    "baseData={}\n",
    "b =('C','G')\n",
    "for s in sources:\n",
    "        d=data[s]\n",
    "        baseData[s]=d[((d['indel']==False) & ((d['ref']==b[0]) | (d['ref']==b[1]))) |\n",
    "                      ((d['hmer_indel_length']>0) & ((d['hmer_indel_nuc']==b[0]) | (d['hmer_indel_nuc']==b[1])))\n",
    "                     ]\n",
    "optTab2,optRes,perfCurve=getPerformance(baseData,categories)\n",
    "for s in sources:\n",
    "    optTab2[s].rename(index={a:'{0} ({1}/{2})'.format(a,b[0],b[1]) for a in optTab2[s].index}, inplace=True)\n",
    "plotPerformance(perfCurve,optRes,categories,source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optTab={}\n",
    "for s in sources:\n",
    "    optTab[s]=pd.concat([optTab1[s], optTab2[s]])\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"all_data_per_base\")\n",
    "df['whole genome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Performance over UG high confidence regions\n",
    "\n",
    "Variant calling peformance exclusing genomic areas where UG performance is poor, i.e:\n",
    "- Homopolymers - runs of length 11 bp and above, padded with four bases around the genomic coordinates,\n",
    "- AT-rich regions - bases where the GC content of the surrounding 40 bases is lower than 5%,\n",
    "- Tandem repeats,\n",
    "- Low mapping quality - regions that are covered by at least 20 reads, but less than 10% of these reads are aligned with mapping quality > 20,\n",
    "- High coverage variability - regions with coverage that is highly variable between samples (std/mean > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtData={}\n",
    "for s in sources:\n",
    "    d=data[s]\n",
    "    if s == 'whole genome':\n",
    "        filtData[s]=d.query(\"ug_hcr==True\").copy()\n",
    "    else:\n",
    "        filtData[s]=d.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "optTab1,optRes,perfCurve=getPerformance(filtData,categories)\n",
    "plotPerformance(perfCurve,optRes,categories,img='hicvg.primary',source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['hmer Indel 4','hmer Indel 5','hmer Indel 6','hmer Indel 7','hmer Indel 8','hmer Indel >8,<=10']\n",
    "optTab2,optRes,perfCurve=getPerformance(filtData,categories)\n",
    "plotPerformance(perfCurve,optRes,categories,img='hicvg.hmers',source={'whole genome':sources['whole genome']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optTab={}\n",
    "for s in sources:\n",
    "    optTab[s]=pd.concat([optTab1[s], optTab2[s]])\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"ug_hcr\")\n",
    "defTable=df.copy()\n",
    "df['whole genome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Homozygous genotyping accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','hmer Indel <=4','hmer Indel >4,<=8','hmer Indel >8,<=10']\n",
    "\n",
    "hmzData={}\n",
    "for s in sources:\n",
    "        d=filtData[s]\n",
    "        hmzData[s]=d[(d['gt_ground_truth']==(1,1)) & (d['classify']!='fn')]\n",
    "optTab,optRes,perfCurve=getPerformance(hmzData,categories)\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"ug_hcr_homozygous\")\n",
    "df['whole genome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance over regions with coverage>=20 and excluding areas with mappability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if 'well_mapped_coverage' in data['whole genome'].columns:\n",
    "    filtData={}\n",
    "    for s in sources:\n",
    "        d=data[s]\n",
    "        filtData[s]=d[(d['well_mapped_coverage']>=20) &\n",
    "                      (d['mappability.0'])\n",
    "                     ]\n",
    "\n",
    "    categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "    optTab1,optRes,perfCurve=getPerformance(filtData,categories)\n",
    "    plotPerformance(perfCurve,optRes,categories,img='hicvg.primary',source={'whole genome':sources['whole genome']})\n",
    "\n",
    "    categories=['hmer Indel 4','hmer Indel 5','hmer Indel 6','hmer Indel 7','hmer Indel 8','hmer Indel >8,<=10']\n",
    "    optTab2,optRes,perfCurve=getPerformance(filtData,categories)\n",
    "    plotPerformance(perfCurve,optRes,categories,img='hicvg.hmers',source={'whole genome':sources['whole genome']})\n",
    "\n",
    "    optTab={}\n",
    "    for s in sources:\n",
    "        optTab[s]=pd.concat([optTab1[s], optTab2[s]])\n",
    "    df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "    df.to_hdf(h5outfile, key=\"good_cvg_data\")\n",
    "    defTable=df.copy()\n",
    "    display(df['whole genome'])\n",
    "else:\n",
    "    print(\"No coverage data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Homozygous genotyping accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "\n",
    "hmzData={}\n",
    "for s in sources:\n",
    "        d=filtData[s]\n",
    "        hmzData[s]=d[(d['gt_ground_truth']==(1,1)) & (d['classify_gt']!='fn')]\n",
    "optTab,optRes,perfCurve=getPerformance(hmzData,categories)\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"good_cvg_data_homozygous\")\n",
    "df['whole genome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  5. Trained with and without Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Trained wo gt - Trained without ground truth </b></li>\n",
    "Random forest model trained on chromosome 9 using known variants in dbSNP and on common fp variants\n",
    "<li><b>Trained with gt - Trained with ground truth</b></li>\n",
    "Simple threshold model trained on chromosome 9 using its own ground truth\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['SNP','Indel','non-hmer Indel','non-hmer Indel w/o LCR','hmer Indel <=4','hmer Indel >4,<=8']\n",
    "optTab1,optRes,perfCurve=getPerformance(data,categories)\n",
    "plotPerformance(perfCurve,optRes,categories,img='all.primary',source={'Trained wo gt':sources['Trained wo gt'], 'Trained with gt': sources['Trained wo gt']},legend=sources[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories=['hmer Indel 4','hmer Indel 5','hmer Indel 6','hmer Indel 7','hmer Indel 8','hmer Indel >8,<=10']\n",
    "optTab2,optRes,perfCurve=getPerformance(data,categories)\n",
    "plotPerformance(perfCurve,optRes,categories,img='all.hmers',source={'Trained wo gt':sources['Trained wo gt'], 'Trained with gt': sources['Trained wo gt']},legend=sources[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2%}'.format\n",
    "sources={'Trained wo gt':sources['Trained wo gt'], 'Trained with gt': sources['Trained wo gt']}\n",
    "optTab={}\n",
    "for s in sources:\n",
    "    optTab[s]=pd.concat([optTab1[s], optTab2[s]])\n",
    "df=pd.concat([optTab[s] for s in sources], axis=1, keys=[s for s in sources])\n",
    "df.to_hdf(h5outfile, key=\"trained_w_wo_gt\")\n",
    "df_all=df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "d=defTable['Trained with gt'][['max recall','recall','precision']]\n",
    "labels=['SNP','Indel','nhmer','nhmer w/o LCR','hmer 2-4','hmer 5-8','hmer 4','hmer 5','hmer 6','hmer 7','hmer 8','hmer 9-10']\n",
    "fig=plt.figure()\n",
    "ax=d.plot()\n",
    "plt.xticks(np.arange(len(d.index)), rotation=30, ha='right')\n",
    "ax.set_xticklabels(labels)\n",
    "plt.ylim([0.4,1.05])\n",
    "plt.grid()\n",
    "plt.title('Cvg>20X, Trained variant calls')\n",
    "plt.tight_layout()\n",
    "nxp.save(fig,imgpref+'summary','png',outdir=imgdir)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
