import gzip
import re
from collections import defaultdict
from collections.abc import Iterable
from os.path import basename

import numpy as np
import pandas as pd
import pyBigWig as pbw
import pyfaidx
from joblib import Parallel, delayed


def _collect_coverage_per_motif(chrom: str, depth_file: str, size: int = 5, n: int = 100):
    """
    Collect coverage per motif from given input file from a single chromosome

    Parameters
    ----------
    chrom
        sequence of the chromosome that matches the input depth_file, all upper case - as obtained by
        pyfaidx.Fasta(reference_fasta)[chr_str][:].seq.upper()
    depth_file
        depth file (generated by coverage_analysis:run_full_coverage_analysis in this repo)
    size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    n
        process 1 in every N positions - makes the code run faster the larger N is (default 100).

    Returns
    -------
        dataframe with column f"motif_{size}" (motif in the reference), and "count" (how many times it was counted)
    -------

    """

    counter = defaultdict(lambda: 0)
    search = re.compile(r"[^ACGTacgt.]").search
    if not depth_file.endswith("bw"):
        open_func = gzip.open if depth_file.endswith(".gz") else open

        with open_func(depth_file) as depth_file_handler:
            for j, line in enumerate(depth_file_handler):
                if j % n != 0:
                    continue
                if isinstance(line, bytes):
                    line = line.decode()
                line = line.strip()
                spl = line.split("\t")
                pos = int(spl[1])
                cov = int(spl[3])
                if pos < 2 * size or cov == 0:
                    continue
                seq = chrom[pos - size - 1 : pos + size]

                if not bool(search(seq)):  # no letters other than ACGT
                    counter[seq] += cov

    else:  # case of bigWig - we fetch by region
        chunk_size = 1000000

        with pbw.open(depth_file) as depth_file_handler:
            assert len(list(depth_file_handler.chroms().keys())) == 1, "Expected single chromosome per bw"
            chrom_name = list(depth_file_handler.chroms().keys())[0]
            chrom_len = depth_file_handler.chroms()[chrom_name]
            start_points = np.arange(0, chrom_len, chunk_size).astype(np.int)

            for s in start_points:
                vals = depth_file_handler.values(chrom_name, s, min(s + chunk_size, chrom_len))
                vals = vals[::n]
                poss = np.arange(s, min(s + chunk_size, chrom_len))[::n]
                for i, pos in enumerate(poss):
                    cov = vals[i]
                    if pos < 2 * size or cov == 0 or np.isnan(cov):
                        continue
                    seq = chrom[pos - size - 1 : pos + size]

                    if not bool(search(seq)):  # no letters other than ACGT
                        counter[seq] += cov

    df = (
        pd.DataFrame(counter.values(), index=counter.keys())
        .reset_index()
        .rename(columns={"index": f"motif_{size}", 0: "count"})
    )

    # edge case of empty dataframe

    if "count" not in df.columns:
        df["count"] = []
    return df


def collect_coverage_per_motif(
    depth_files,
    reference_fasta: str,
    outfile: str = None,
    show_stats: bool = False,
    n_jobs: int = -1,
    size: int = 5,
    downsampling_ratio: int = 100,
):
    """
    Collect coverage per motif from given input files, output is a set of dataframes with motif of a given size and
    columns "count", which is the number of occurences of this motif in the sampled data, and "coverage" which is an
    extrapolation to the full data set

    Parameters
    ----------
    :param: depth_files
        dictionary of depth files in BW or bed format
        (generated by coverage_analysis:run_full_coverage_analysis in this repo),
        where keys are the chromosomes and values are the files (window size 1)
        alternatively, this can be an Iterable of files, but the filenames must follow the convention where each file
        contains reads from one chromosome only, and the file basename contains the chromosome name surrounded by dots,
        i.e. /path/to/file/filename.chr1.bw
    :param: reference_fasta

    :param: outfile
        output file to save to, hdf format - each motif of size num is save under the key "motif_{num}", if None
        (default) output is not saved
    :param: show_stats
        print statistics for each calculated motif, default False
    :param: n_jobs
        number of processes to run in parallel, -1 (default) for maximal possible number
    :param: size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    :param: downsampling_ratio
        process 1 in every N=downsampling_ratio positions - makes the code run faster the larger N is (default 100).
        In the output dataframe df, df['coverage'] = df['count'] * N

    :return:
    -------
    motif coverage dataframe

    :raises ValueError: may be raised
    """

    if not isinstance(depth_files, dict):
        if not isinstance(depth_files, Iterable):
            raise ValueError(f"Expected dictionary or Iterable, got:\n{depth_files}")

        def _extract_chrom(fname):
            for x in basename(fname).split("."):
                if x.startswith("chr"):
                    return x
            raise ValueError(f"Could not figure out chromosome of the file {fname}")

        # convert to dictionary
        depth_files = {_extract_chrom(fname): fname for fname in depth_files}

    df = pd.concat(
        Parallel(n_jobs=n_jobs)(
            delayed(_collect_coverage_per_motif)(
                chrom=pyfaidx.Fasta(reference_fasta)[chr_str][:].seq.upper(),
                depth_file=depth_file,
                size=size,
                n=downsampling_ratio,
            )
            for chr_str, depth_file in depth_files.items()
        )
    )
    df = df.groupby(f"motif_{size}").sum()
    df = df.reset_index()
    for j in list(range(size))[::-1]:
        df[f"motif_{j}"] = df[f"motif_{j + 1}"].str.slice(1, -1)

    df = df[[f"motif_{j}" for j in range(size + 1)] + ["count"]]
    df["coverage"] = df["count"] * downsampling_ratio
    if outfile is not None:
        for j in range(size + 1):
            df.groupby(f"motif_{j}").agg({"count": "sum", "coverage": "sum"}).to_hdf(outfile, key=f"motif_{j}")
    if show_stats:
        print(
            pd.concat(
                (
                    df.groupby(f"motif_{j}").agg({"count": "sum"}).describe().rename(columns={"count": f"motif_{j}"})
                    for j in range(size + 1)
                ),
                axis=1,
            )
        )
    return df
