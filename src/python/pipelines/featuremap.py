import sys
from tqdm import tqdm
import re
from collections import defaultdict
import pandas as pd
import numpy as np
from joblib import Parallel, delayed
import pysam
import pyfaidx
import itertools
import argparse
import gzip
from os.path import dirname, basename
from collections.abc import Iterable

path = dirname(dirname(dirname(__file__)))
if path not in sys.path:
    sys.path.append(path)
from python.utils import revcomp, generateKeyFromSequence
from python.modules.variant_annotation import get_motif_around
from python.auxiliary.format import (
    CHROM_DTYPE,
    CYCLE_SKIP_DTYPE,
    CYCLE_SKIP,
    POSSIBLE_CYCLE_SKIP,
    NON_CYCLE_SKIP,
)


def _collect_coverage_per_motif(
    chr_str: CHROM_DTYPE,
    depth_file: str,
    reference_fasta: str,
    size: int = 5,
    N: int = 100,
):
    """
    Collect coverage per motif from given input file from a single chromosome

    Parameters
    ----------
    chr_str
        chromosome, must match the input depth_file
    depth_file
        depth file (generated by coverage_analysis:run_full_coverage_analysis in this repo)
    reference_fasta

    size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    N
        process 1 in every N positions - makes the code run faster the larger N is (default 100).

    Returns
        dataframe with column f"motif_{size}" (motif in the reference), and "count" (how many times it was counted)
    -------

    """
    chrom = pyfaidx.Fasta(reference_fasta)[chr_str]

    counter = defaultdict(lambda: 0)
    search = re.compile(r"[^ACGTacgt.]").search
    open_func = gzip.open if depth_file.endswith(".gz") else open
    with open_func(depth_file) as f:
        for j, line in enumerate(f):
            if j % N != 0:
                continue
            if isinstance(line, bytes):
                line = line.decode()
            line = line.strip()
            spl = line.split("\t")
            pos = int(spl[1])
            cov = int(spl[3])
            if pos < 2 * size or cov == 0:
                continue
            seq = chrom[pos - size - 1 : pos + size].seq.upper()

            if not bool(search(seq)):  # no letters other than ACGT
                counter[seq] += cov

    df = (
        pd.DataFrame(counter.values(), counter.keys())
        .reset_index()
        .rename(columns={"index": f"motif_{size}", 0: "count"})
    )

    return df


def collect_coverage_per_motif(
    depth_files,
    reference_fasta: str,
    outfile: str = None,
    show_stats: bool = False,
    n_jobs: int = -1,
    size: int = 5,
    N: int = 100,
):
    """
    Collect coverage per motif from given input files, output is a set of dataframes with motif of a given size and
    columns "count", which is the number of occurences of this motif in the sampled data, and "coverage" which is an
    extrapolation to the full data set

    Parameters
    ----------
    depth_files
        dictionary of depth files (generated by coverage_analysis:run_full_coverage_analysis in this repo), where keys
        are the chromosomes and values are the files (window size 1)
        alternatively, this can be an Iterable of files, but the filenames must follow the convention where each file
        contains reads from one chromosome only, and the file basename contains the chromosome name surrounded by dots,
        i.e. /path/to/file/filename.chr1.bed
    reference_fasta

    outfile
        output file to save to, hdf format - each motif of size num is save under the key "motif_{num}", if None
        (default) output is not saved
    show_stats
        print statistics for each calculated motif, default False
    n_jobs
        number of processes to run in parallel, -1 (default) for maximal possible number
    size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    N
        process 1 in every N positions - makes the code run faster the larger N is (default 100). in the output
        dataframe df, df['coverage'] = df['count'] * N

    Returns
    -------
    motif coverage dataframe

    """

    if not isinstance(depth_files, dict):
        if not isinstance(depth_files, Iterable):
            raise ValueError(f"Expected dictionary or Iterable, got:\n{depth_files}")

        def _extract_chrom(fname):
            for x in basename(fname).split("."):
                if x.startswith("chr"):
                    return x
            raise ValueError(f"Could not figure out chromosome of the file {fname}")

        # convert to dictionary
        depth_files = {_extract_chrom(fname): fname for fname in depth_files}

    df = pd.concat(
        Parallel(n_jobs=n_jobs)(
            delayed(_collect_coverage_per_motif)(
                chr_str, depth_file, reference_fasta, size=size, N=N
            )
            for chr_str, depth_file in depth_files.items()
        )
    )
    df = df.groupby(f"motif_{size}").sum()
    df = df.reset_index()
    for j in list(range(size))[::-1]:
        df[f"motif_{j}"] = df[f"motif_{j + 1}"].str.slice(1, -1)
    df = df[[f"motif_{j}" for j in range(size + 1)] + ["count"]]
    df["coverage"] = df["count"] * N
    if outfile is not None:
        for j in range(size + 1):
            df.groupby(f"motif_{j}").agg({"count": "sum", "coverage": "sum"}).to_hdf(
                outfile, key=f"motif_{j}"
            )
    if show_stats:
        print(
            pd.concat(
                (
                    df.groupby(f"motif_{j}")
                    .agg({"count": "sum"})
                    .describe()
                    .rename(columns={"count": f"motif_{j}"})
                    for j in range(size + 1)
                ),
                axis=1,
            )
        )
    return df


def featuremap_to_dataframe(
    featuremap_vcf: str,
    output_file: str = None,
    reference_fasta: str = None,
    motif_length: int = 4,
    report_read_orientation: bool = True,
    x_fields: list = None,
    show_progress_bar: bool = False,
    flow_order: str = "TGCA",
):
    """
    Converts featuremap in vcf format to dataframe
    if reference_fasta, annotates with motifs of length "motif_length"
    if flow_order is also given, annotates cycle skip status per entry

    Parameters
    ----------
    featuremap_vcf
        featuremap file generated by "gatk FeatureMap"
    output_file
        file path to save
    reference_fasta
        reference genome used to generate the bam that the featuremap was generated from, if not None (default) the
        entries in the featuremap are annorated for motifs with the length of the next parameters from either side
    motif_length
        default 4
    report_read_orientation
        featuremap entries are reported for the sense strand regardless of read diretion. If True (default), the ref and
        alt columns are reverse complemented for reverse strand reads (also motif if calculated).
    x_fields
        fields to extract from featuremap, if default (None) those are extracted:
        "X-CIGAR", "X-EDIST", "X-FC1", "X-FC2", "X-FILTERED-COUNT", "X-FLAGS", "X-LENGTH", "X-MAPQ", "X-READ-COUNT",
        "X-RN", "X-SCORE", "rq",
    show_progress_bar
        displays tqdm progress bar of number of lines read (not in percent)
    flow_order
        flow order

    Returns
    -------

    """
    if x_fields is None:
        x_fields = [
            "X-CIGAR",
            "X-EDIST",
            "X-FC1",
            "X-FC2",
            "X-FILTERED-COUNT",
            "X-FLAGS",
            "X-LENGTH",
            "X-MAPQ",
            "X-READ-COUNT",
            "X-RN",
            "X-SCORE",
            "rq",
        ]

    with pysam.VariantFile(featuremap_vcf) as f:
        vfi = map(
            lambda x: defaultdict(
                lambda: None,
                x.info.items()
                + [
                    ("CHROM", x.chrom),
                    ("POS", x.pos),
                    ("REF", x.ref),
                    ("ALT", x.alts[0]),
                ]
                + [(xf, x.info[xf]) for xf in x_fields],
            ),
            f,
        )
        columns = ["chrom", "pos", "ref", "alt"] + x_fields
        df = pd.DataFrame(
            (
                [x[y.upper() if y != "rq" else y] for y in columns]
                for x in tqdm(
                    vfi,
                    disable=not show_progress_bar,
                    desc="Reading and converting vcf file",
                )
            ),
            columns=columns,
        )

    if report_read_orientation:
        is_reverse = ~(df["X-FLAGS"] & 16).astype(bool)
        for c in ["ref", "alt"]:  # reverse value to match the read direction
            df[c] = df[c].where(is_reverse, df[c].apply(revcomp))

    if reference_fasta is not None:
        df = get_motif_around(
            df.assign(indel=False), motif_length, reference_fasta
        ).drop(columns=["indel"])

        if report_read_orientation:
            left_motif_reverse = df["left_motif"].apply(revcomp)
            right_motif_reverse = df["right_motif"].apply(revcomp)
            df["left_motif"] = df["left_motif"].where(is_reverse, right_motif_reverse)
            df["right_motif"] = df["right_motif"].where(is_reverse, left_motif_reverse)

        df["ref_motif"] = (
            df["left_motif"].str.slice(-1)
            + df["ref"]
            + df["right_motif"].str.slice(0, 1)
        )
        df["alt_motif"] = (
            df["left_motif"].str.slice(-1)
            + df["alt"]
            + df["right_motif"].str.slice(0, 1)
        )
        df = df.astype(
            {
                "chrom": CHROM_DTYPE,
                "ref": "category",
                "alt": "category",
                "ref_motif": "category",
                "alt_motif": "category",
                "left_motif": "category",
                "right_motif": "category",
            }
        )

        if flow_order is not None:
            df_cskp = get_cycle_skip_dataframe(flow_order=flow_order)
            df = df.set_index(["ref_motif", "alt_motif"]).join(df_cskp).reset_index()

    df = df.set_index(["chrom", "pos"]).sort_index()
    if output_file is None:
        if featuremap_vcf.endswith(".vcf.gz"):
            output_file = featuremap_vcf[: -len(".vcf.gz")] + ".parquet"
        else:
            output_file = featuremap_vcf + ".parquet"
    df.to_parquet(output_file)
    return df


def determine_cycle_skip_status(ref: str, alt: str, flow_order: str):
    """return the cycle skip status, expects input of ref and alt sequences composed of 3 bases where only the 2nd base
    differs"""
    if (
        len(ref) != 3
        or len(alt) != 3
        or ref[0] != alt[0]
        or ref[2] != alt[2]
        or ref == alt
    ):
        raise ValueError(
            f"""Invalid inputs ref={ref}, alt={alt}
expecting input of ref and alt sequences composed of 3 bases where only the 2nd base differs"""
        )
    ref_key = np.trim_zeros(generateKeyFromSequence(ref, flow_order), "f")
    alt_key = np.trim_zeros(generateKeyFromSequence(alt, flow_order), "f")
    if len(ref_key) != len(alt_key):
        return CYCLE_SKIP
    else:
        for r, a in zip(ref_key, alt_key):
            if (r != a) and ((r == 0) or (a == 0)):
                return POSSIBLE_CYCLE_SKIP
        return NON_CYCLE_SKIP


def get_cycle_skip_dataframe(flow_order: str = "TGCA"):
    ind = pd.MultiIndex.from_tuples(
        [
            x
            for x in itertools.product(
                ["".join(x) for x in itertools.product(["A", "C", "G", "T"], repeat=3)],
                ["A", "C", "G", "T"],
            )
            if x[0][1] != x[1]
        ],
        names=["ref_motif", "alt_motif"],
    )
    df_cskp = pd.DataFrame(index=ind).reset_index()
    df_cskp["alt_motif"] = (
        df_cskp["ref_motif"].str.slice(0, 1)
        + df_cskp["alt_motif"]
        + df_cskp["ref_motif"].str.slice(-1)
    )
    df_cskp["cycle_skip_status"] = df_cskp.apply(
        lambda row: determine_cycle_skip_status(
            row["ref_motif"], row["alt_motif"], flow_order
        ),
        axis=1,
    ).astype(CYCLE_SKIP_DTYPE)
    return df_cskp.set_index(["ref_motif", "alt_motif"])


def merge_featuremap_dataframes(dataframes: list, outfile: str, n_jobs: int = 1):
    df = pd.concat(
        Parallel(n_jobs=n_jobs)(delayed(pd.read_parquet)(f) for f in dataframes)
    )
    df = df.sort_index()
    df.to_parquet(outfile)
    return df


def call_featuremap_to_dataframe(args_in):
    if args_in.input is None:
        raise ValueError("No input provided")
    featuremap_to_dataframe(
        featuremap_vcf=args_in.input,
        output_file=args_in.output,
        reference_fasta=args_in.reference_fasta,
        motif_length=args_in.motif_length,
        report_read_orientation=not args_in.report_sense_strand_bases,
        show_progress_bar=args_in.show_progress_bar,
        flow_order=args_in.flow_order,
    )
    sys.stdout.write("DONE\n")


def call_merge_featuremap_dataframes(args_in):
    if args_in.input is None:
        raise ValueError("No input provided")
    merge_featuremap_dataframes(
        dataframes=args_in.input, outfile=args_in.output, n_jobs=args_in.jobs,
    )
    sys.stdout.write("DONE\n")


def call_collect_coverage_per_motif(args_in):
    if args_in.input is None:
        raise ValueError("No input provided")
    collect_coverage_per_motif(
        depth_files=args_in.input,
        reference_fasta=args_in.reference_fasta,
        outfile=args_in.output,
        show_stats=args_in.show_stats,
        n_jobs=args_in.jobs,
        size=args_in.motif_length,
        N=args_in.N,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()

    parser_featuremap_to_dataframe = subparsers.add_parser(
        name="to_dataframe", description="""Convert featuremap to pandas dataframe""",
    )
    parser_concat_dataframes = subparsers.add_parser(
        name="concat_dataframes",
        description="""Concat featuremap pandas dataframe created on different intevals""",
    )
    parser_coverage_per_motif = subparsers.add_parser(
        name="collect_coverage_per_motif",
        description="""Collect coverage per motif from a collection of depth files""",
    )

    parser_featuremap_to_dataframe.add_argument(
        "-i", "--input", type=str, required=True, help="input featuremap file",
    )
    parser_featuremap_to_dataframe.add_argument(
        "-o",
        "--output",
        type=str,
        default=None,
        help="""Path to which output dataframe will be written, if None a file with the same name as input and 
".parquet" extension will be created""",
    )
    parser_featuremap_to_dataframe.add_argument(
        "-r",
        "--reference_fasta",
        type=str,
        help="""reference fasta, only required for motif annotation
most likely gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta but it must be localized""",
    )
    parser_featuremap_to_dataframe.add_argument(
        "-f",
        "--flow_order",
        type=str,
        required=False,
        default=None,
        help="""flow order - required for cycle skip annotation but not mandatory""",
    )
    parser_featuremap_to_dataframe.add_argument(
        "-m",
        "--motif_length",
        type=int,
        default=4,
        help="motif length to annotate the vcf with",
    )
    parser_featuremap_to_dataframe.add_argument(
        "--report_sense_strand_bases",
        default=False,
        action="store_true",
        help="if True, the ref, alt, and motifs will be reported according to the sense strand and not according to the read orientation",
    )
    parser_featuremap_to_dataframe.add_argument(
        "--show_progress_bar",
        default=False,
        action="store_true",
        help="show progress bar (tqdm)",
    )

    parser_featuremap_to_dataframe.set_defaults(func=call_featuremap_to_dataframe)

    parser_concat_dataframes.add_argument(
        "input", nargs="+", type=str, help="input featuremap files",
    )
    parser_concat_dataframes.add_argument(
        "-o",
        "--output",
        type=str,
        default=None,
        required=True,
        help="""Path to which output dataframe will be written, if None a file with the same name as input and 
    ".parquet" extension will be created""",
    )
    parser_concat_dataframes.add_argument(
        "-j",
        "--jobs",
        type=int,
        default=1,
        help="Number of jobs to run in parallel (default 1, -1 for max)",
    )

    parser_coverage_per_motif.set_defaults(func=call_merge_featuremap_dataframes)

    parser_coverage_per_motif.add_argument(
        "input", nargs="+", type=str, help="input depth files",
    )
    parser_coverage_per_motif.add_argument(
        "-o",
        "--output",
        type=str,
        default=None,
        required=True,
        help="""Path to which output dataframe will be written in hdf format""",
    )
    parser_coverage_per_motif.add_argument(
        "-r",
        "--reference_fasta",
        type=str,
        help="""reference fasta, only required for motif annotation
    most likely gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta but it must be localized""",
    )
    parser_coverage_per_motif.add_argument(
        "-N",
        type=int,
        default=4,
        help="""Process 1 in every N positions - makes the code run faster the larger N is (default 100). In the output
dataframe df, df['coverage'] = df['count'] * N""",
    )
    parser_coverage_per_motif.add_argument(
        "-m",
        "--motif_length",
        type=int,
        default=4,
        help="Maximal motif length to collect coverage for",
    )
    parser_coverage_per_motif.add_argument(
        "--show_stats",
        default=False,
        action="store_true",
        help="Print motif statistics",
    )
    parser_coverage_per_motif.add_argument(
        "-j",
        "--jobs",
        type=int,
        default=-1,
        help="Number of jobs to run in parallel (default -1 for max)",
    )
    parser_coverage_per_motif.set_defaults(func=call_collect_coverage_per_motif)

    args = parser.parse_args()
    args.func(args)
