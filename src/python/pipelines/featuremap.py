import sys
from tqdm import tqdm
import re
from collections import defaultdict
import pandas as pd
import numpy as np
from joblib import Parallel, delayed
import pysam
import pyfaidx
import itertools
import argparse

from python.utils import revcomp, generateKeyFromSequence
from python.modules.variant_annotation import get_motif_around
from python.auxiliary.format import CHROM_DTYPE

CYCLE_SKIP = "cycle-skip"
POSSIBLE_CYCLE_SKIP = "possible-cycle-skip"
NON_CYCLE_SKIP = "non-skip"


def _collect_coverage_per_motif(chr_str, depth_file, reference_fasta, size=5, N=100):
    """
    Collect coverage per motif from given input file from a single chromosome

    Parameters
    ----------
    chr_str
        chromosome
    depth_file
        depth file (generated by coverage_analysis:run_full_coverage_analysis in this repo)
    reference_fasta

    size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    N
        process 1 in every N positions - makes the code run faster the larger N is (default 100). in the output
        dataframe df, df['coverage'] = df['count'] * N

    Returns
    -------

    """
    chrom = pyfaidx.Fasta(reference_fasta)[chr_str]

    counter = defaultdict(lambda: 0)
    search = re.compile(r"[^ACGTacgt.]").search
    with open(depth_file) as f:
        for j, line in enumerate(f):
            if j % N != 0:
                continue
            spl = line.split("\t")
            pos = int(spl[1])
            cov = int(spl[3])
            if pos < 2 * size or cov == 0:
                continue
            seq = chrom[pos - size - 1 : pos + size].seq.upper()

            if not bool(search(seq)):  # no letters other than ACGT
                counter[seq] += cov

    df = (
        pd.DataFrame(counter.values(), counter.keys())
        .reset_index()
        .rename(columns={"index": f"motif_{size}", 0: "count"})
    )

    return df


def collect_coverage_per_motif(
    depth_files_dict,
    reference_fasta,
    outfile=None,
    show_stats=False,
    n_jobs=-1,
    size=5,
    N=100,
):
    """
    Collect coverage per motif from given input files, output is a set of dataframes with motif of a given size and
    columns "count", which is the number of occurences of this motif in the sampled data, and "coverage" which is an
    extrapolation to the full data set

    Parameters
    ----------
    depth_files_dict
        dictionary of depth files (generated by coverage_analysis:run_full_coverage_analysis in this repo), where keys
        are the chromosomes and values are the files (window size 1)
    reference_fasta

    outfile
        output file to save to, hdf format - each motif of size num is save under the key "motif_{num}", if None
        (default) output is not saved
    show_stats
        print statistics for each calculated motif, default False
    n_jobs
        number of processes to run in parallel, -1 (default) for maximal possible number
    size
        maximal motif size, deault 5 - all motifs below that number until 0 are also calculated
    N
        process 1 in every N positions - makes the code run faster the larger N is (default 100). in the output
        dataframe df, df['coverage'] = df['count'] * N

    Returns
    -------
    motif coverage dataframe

    """
    df = pd.concat(
        Parallel(n_jobs=n_jobs)(
            delayed(_collect_coverage_per_motif)(
                chr_str, depth_file, reference_fasta, size=size, N=N
            )
            for chr_str, depth_file in depth_files_dict.items()
        )
    )
    df = df.groupby(f"motif_{size}").sum()
    df = df.reset_index()
    for j in list(range(size))[::-1]:
        df[f"motif_{j}"] = df[f"motif_{j + 1}"].str.slice(1, -1)
    df = df[[f"motif_{j}" for j in range(size + 1)] + ["count"]]
    df["coverage"] = df["count"] * N
    if outfile is not None:
        for j in range(size + 1):
            df.groupby(f"motif_{j}").agg({"count": "sum", "coverage": "sum"}).to_hdf(
                outfile, key=f"motif_{j}"
            )
    if show_stats:
        print(
            pd.concat(
                (
                    df.groupby(f"motif_{j}")
                    .agg({"count": "sum"})
                    .describe()
                    .rename(columns={"count": f"motif_{j}"})
                    for j in range(size + 1)
                ),
                axis=1,
            )
        )
    return df


def featuremap_to_dataframe(
    featuremap_vcf,
    output_file=None,
    reference_fasta=None,
    motif_length=4,
    report_read_orientation=True,
    x_fields=None,
    show_progress_bar=False,
    flow_order="TGCA",
):
    """
    Converts featuremap in vcf format to dataframe
    if reference_fasta, annotates with motifs of length "motif_length"
    if flow_order is also given, annotates cycle skip status per entry

    Parameters
    ----------
    featuremap_vcf
        featuremap file generated by "gatk FeatureMap"
    output_file
        file path to save
    reference_fasta
        reference genome used to generate the bam that the featuremap was generated from, if not None (default) the
        entries in the featuremap are annorated for motifs with the length of the next parameters from either side
    motif_length
        default 4
    report_read_orientation
        featuremap entries are reported for the sense strand regardless of read diretion. If True (default), the ref and
        alt columns are reverse complemented for reverse strand reads (also motif if calculated).
    x_fields
        fields to extract from featuremap, if default (None) those are extracted:
        "X-CIGAR", "X-EDIST", "X-FC1", "X-FC2", "X-FILTERED-COUNT", "X-FLAGS", "X-LENGTH", "X-MAPQ", "X-READ-COUNT",
        "X-RN", "X-SCORE", "rq",
    show_progress_bar
        displays tqdm progress bar of number of lines read (not in percent)
    flow_order
        flow order

    Returns
    -------

    """
    if x_fields is None:
        x_fields = [
            "X-CIGAR",
            "X-EDIST",
            "X-FC1",
            "X-FC2",
            "X-FILTERED-COUNT",
            "X-FLAGS",
            "X-LENGTH",
            "X-MAPQ",
            "X-READ-COUNT",
            "X-RN",
            "X-SCORE",
            "rq",
        ]

    with pysam.VariantFile(featuremap_vcf) as f:
        vfi = map(
            lambda x: defaultdict(
                lambda: None,
                x.info.items()
                + [
                    ("CHROM", x.chrom),
                    ("POS", x.pos),
                    ("REF", x.ref),
                    ("ALT", x.alts[0]),
                ]
                + [(xf, x.info[xf]) for xf in x_fields],
            ),
            f,
        )
        columns = ["chrom", "pos", "ref", "alt"] + x_fields
        df = pd.DataFrame(
            (
                [x[y.upper()] for y in columns]
                for x in tqdm(
                    vfi,
                    disable=not show_progress_bar,
                    desc="Reading and converting vcf file",
                )
            ),
            columns=columns,
        )

    if report_read_orientation:
        is_reverse = ~(df["X-FLAGS"] & 16).astype(bool)
        for c in ["ref", "alt"]:  # reverse value to match the read direction
            df[c] = df[c].where(is_reverse, df[c].apply(revcomp))

    if reference_fasta is not None:
        df = get_motif_around(
            df.assign(indel=False), motif_length, reference_fasta
        ).drop(columns=["indel"])

        if report_read_orientation:
            left_motif_reverse = df["left_motif"].apply(revcomp)
            right_motif_reverse = df["right_motif"].apply(revcomp)
            df["left_motif"] = df["left_motif"].where(is_reverse, right_motif_reverse)
            df["right_motif"] = df["right_motif"].where(is_reverse, left_motif_reverse)

        df["ref_motif"] = (
            df["left_motif"].str.slice(-1)
            + df["ref"]
            + df["right_motif"].str.slice(0, 1)
        )
        df["alt_motif"] = (
            df["left_motif"].str.slice(-1)
            + df["alt"]
            + df["right_motif"].str.slice(0, 1)
        )
        df = df.astype(
            {
                "chrom": CHROM_DTYPE,
                "ref": "category",
                "alt": "category",
                "ref_motif": "category",
                "alt_motif": "category",
                "left_motif": "category",
                "right_motif": "category",
            }
        )

        if flow_order is not None:
            df_cskp = get_cycle_skip_dataframe(flow_order=flow_order)
            df = df.join(df_cskp)

    if output_file is not None:
        df.to_parquet(output_file)
    return df


def determine_cycle_skip_status(ref, alt, flow_order):
    """return the cycle skip status, expects input of ref and alt sequences composed of 3 bases where only the 2nd base
    differs"""
    if (
        len(ref) != 3
        or len(alt) != 3
        or ref[0] != alt[0]
        or ref[2] != alt[2]
        or ref == alt
    ):
        raise ValueError(
            f"""Invalid inputs ref={ref}, alt={alt}
expecting input of ref and alt sequences composed of 3 bases where only the 2nd base differs"""
        )
    ref_key = np.trim_zeros(generateKeyFromSequence(ref, flow_order), "f")
    alt_key = np.trim_zeros(generateKeyFromSequence(alt, flow_order), "f")
    if len(ref_key) != len(alt_key):
        return CYCLE_SKIP
    else:
        for r, a in zip(ref_key, alt_key):
            if (r != a) and ((r == 0) or (a == 0)):
                return POSSIBLE_CYCLE_SKIP
        return NON_CYCLE_SKIP


def get_cycle_skip_dataframe(flow_order="TGCA"):
    ind = pd.MultiIndex.from_tuples(
        [
            x
            for x in itertools.product(
                ["".join(x) for x in itertools.product(["A", "C", "G", "T"], repeat=3)],
                ["A", "C", "G", "T"],
            )
            if x[0][1] != x[1]
        ],
        names=["ref_motif", "alt_motif"],
    )
    df_cskp = pd.DataFrame(index=ind).reset_index()
    df_cskp["alt_motif"] = (
        df_cskp["ref_motif"].str.slice(0, 1)
        + df_cskp["alt_motif"]
        + df_cskp["ref_motif"].str.slice(-1)
    )
    df_cskp["cycle_skip_status"] = df_cskp.apply(
        lambda row: determine_cycle_skip_status(
            row["ref_motif"], row["alt_motif"], flow_order
        ),
        axis=1,
    ).astype("category")
    return df_cskp.set_index(["ref_motif", "alt_motif"])


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()
    #
    # parser_full_analysis = subparsers.add_parser(
    #     name="full_analysis",
    #     description="""Run full coverage analysis of an aligned bam/cram file""",
    # )
    # parser_full_analysis.add_argument(
    #     "-i", "--input", type=str, help="input bam or cram file ",
    # )
